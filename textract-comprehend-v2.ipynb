{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Textract and Amazon Comprehend AI Services\n",
    "### Example on extracting insights from a PDF Document\n",
    "\n",
    "\n",
    "## Contents \n",
    "1. [Background](#Background)\n",
    "1. [Notes and Configuration](#Notes-and-Configuration)\n",
    "1. [Functions](#Functions)\n",
    "1. [Amazon Textract](#Amazon-Textract)\n",
    "1. [Amazon Comprehend](#Amazon-Comprehend)\n",
    "1. [Key Phrase Extraction](#Key-Phrase-Extraction)\n",
    "1. [Sentiment Analysis](#Sentiment-Analysis)\n",
    "1. [Entity Recognition](#Entity-Recognition)\n",
    "1. [PII Entity Recognition](#PII-Entity-Recognition)\n",
    "1. [Topic Modeling](#Topic-Modeling)\n",
    "\n",
    "\n",
    "  \n",
    "## Background\n",
    "The goal of this exercise is to learn some insights from an existing PDF document. This is done by using Amazon Textract to extract the text from the document. This text is then analyzed by several Amazon Comprehend services to produce some insights about the document.  \n",
    "\n",
    "The PDF document used in this example is a compiled list of tweets or other social media posts. Each post is separated by a URL that points to that posting. When the text is extracted from the PDF document, the text is re-assembled into a single line of text which is the full text of the tweet or post. The resulting text file contains one tweet/post per line.\n",
    "\n",
    "## Notes and Configuration\n",
    "* Kernel `Python 3 (Data Science)` works well with this notebook\n",
    "* The CSV results files produced by this script use the pipe '|' symbol as a delimiter. When viewing these files in SageMaker Studio, be sure and change the Delimiter to 'pipe'.\n",
    "\n",
    "\n",
    "#### Regarding IAM Roles and Permissions:\n",
    "\n",
    "Within SageMaker Studio, each SageMaker User has an IAM Role known as the `SageMaker Execution Role`. Each Notebook for this user will run with this Role and the Permissions specified by this Role. The name of this Role can be found in the Details section of each SageMaker User in the AWS Console.\n",
    "\n",
    "For the code which runs in this notebook, the `SageMaker Execution Role` needs additional permissions to allow it to use Amazon Textract and Amazon Comprehend. In the AWS Console, navigate to the IAM service and add these two services to your SageMaker Execution Role:\n",
    "- AmazonTextractFullAccess\n",
    "- AmazonComprehendFullAccess\n",
    "\n",
    "Also, an Amazon Comprehend service Role needs to be created to grant Amazon Comprehend read access to your input data.  \n",
    "Click the AttachPolicies button and add AmazonS3FullAccess. Complete this step and name your role as follows\n",
    "\n",
    "`asyncS3ComprehendServiceRole`\n",
    "\n",
    "Once you have created the role, copy the ARN (it will be in the format `arn:aws:iam::<AccountID>:role/asyncS3ComprehendServiceRole`)\n",
    "\n",
    "###When creating this new Role, the default Policies are sufficient (i.e., no other Policies need to be added/modified).\n",
    "\n",
    "Lastly, the `SageMaker Execution Role` must be allowed to Pass the Comprehend Service Role. To allow this, you must attach a Policy to the `SageMaker Execution Role`. Below, the Resource entry is the ARN of the Comprehend service Role which you created. You can either create this as a new Policy and attach it or add it as an in-line Policy.\n",
    "\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"iam:GetRole\",\n",
    "                    \"iam:PassRole\"\n",
    "                ],\n",
    "                \"Resource\": \"<ENTER YOUR ARN HERE>\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Set some variables that will be used throughout this example\n",
    "\n",
    "NOTE: Update the comprehend_role to be the ARN you copied in the step above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'us-east-1'\n",
    "\n",
    "# change this to an existing S3 bucket in your AWS account\n",
    "bucket = '<ENTER YOUR BUCKET NAME HERE>'\n",
    "\n",
    "# this is the role that will be used by the async call to Comprehend TopicModeling at the end of this lab \n",
    "comprehend_role = '<ENTER YOUR ARN HERE>'\n",
    "\n",
    "# this is where the various analysis results files will be stored on the local file system of this SageMaker instance\n",
    "results_dir = './results'\n",
    "!mkdir -p $results_dir\n",
    "\n",
    "# the pdf file to be analyzed by Textract\n",
    "textract_src_filename = 'Alabama2.pdf'\n",
    "\n",
    "# the name of the file where the JSON results from Textract are saved\n",
    "json_textract_results_filename = f'{results_dir}/textract-results.json'\n",
    "\n",
    "# the post-processed results of the JSON results\n",
    "textract_results_filename = f'{results_dir}/textract-results.txt'\n",
    "\n",
    "# the post-processed results of the JSON results where each line is less than 5000 chars\n",
    "trimmed_textract_results_filename = f'{results_dir}/trimmed_textract-results.txt'\n",
    "\n",
    "# the results of Amazon Comprehend - Key Phrases detection\n",
    "comprehend_keyphrases_results_filename = f'{results_dir}/comp-keyphrases.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Sentiment Analysis\n",
    "comprehend_sentiments_results_filename = f'{results_dir}/comp-sentiment.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Entities Detection\n",
    "comprehend_entities_results_filename = f'{results_dir}/comp-entities.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Entities Detection\n",
    "comprehend_pii_entities_results_filename = f'{results_dir}/comp-pii_entities.csv'\n",
    "\n",
    "# the results of Amazon Comprehend - Topics Detection\n",
    "comprehend_topics_results_filename = f'{results_dir}/comp-topics.csv'\n",
    "\n",
    "# this is the IAM Role that defines which permissions this SageMaker instance has\n",
    "sm_execution_role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets download the dataset and copy it to our bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://lofgren.house.gov/sites/lofgren.house.gov/files/Alabama2.pdf\n",
    "!aws s3 cp Alabama2.pdf s3://{bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Functions\n",
    "The following is a convenience function to calculate frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "CalcFrequencies()\n",
    "Input: dict with keys and numeric values\n",
    "Returns: dict with the same keys and numeric frequency\n",
    "'''\n",
    "def CalcFrequencies(di):\n",
    "    \n",
    "    freq = {}\n",
    "    \n",
    "    sum = 0\n",
    "    for d in di:\n",
    "        sum += di[d]\n",
    "    \n",
    "    for d in di:\n",
    "        freq[d] = di[d]/sum\n",
    "\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Textract\n",
    "Amazon Textract is a machine learning service that automatically extracts text, handwriting and data from scanned documents that goes beyond simple optical character recognition (OCR) to identify, understand, and extract data from forms and tables.  \n",
    "  \n",
    "In the next few cells the following steps will be performed:\n",
    "1. A specified PDF document will be uploaded to Amazon S3 to be analyzed by Amazon Textract.  \n",
    "1. The result of this analysis is a JSON file with each element containing details about a specific instance of text in the PDF.  \n",
    "1. This JSON file is copied from S3 to this local SageMaker instance.  \n",
    "1. The JSON file is then read and post-processed to produce a text file with one tweet (or other social media post) per line.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boto3 session\n",
    "# this session will be used for the remainder of this notebook\n",
    "session = boto3.Session(region_name=region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Textract Job\n",
    "textract_client = session.client('textract')\n",
    "\n",
    "response = textract_client.start_document_text_detection(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': bucket,\n",
    "            'Name': textract_src_filename\n",
    "        }\n",
    "    })\n",
    "\n",
    "jobId = response['JobId']\n",
    "\n",
    "print('Started Textract job at %s' % (time.ctime()))\n",
    "print('JobId: %s' % (jobId))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current job status\n",
    "response = textract_client.get_document_text_detection(JobId=jobId)\n",
    "response['JobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now extract the results as a JSON List\n",
    "\n",
    "pages = []\n",
    "if response['JobStatus'] == 'SUCCEEDED':\n",
    "    while('NextToken' in response):\n",
    "        pages.append(response)\n",
    "        response = textract_client.get_document_text_detection(JobId=jobId, NextToken=response['NextToken'])\n",
    "    pages.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the Textract responses, looking for the LINE and WORD entries and write out to a file\n",
    "\n",
    "with open(textract_results_filename, 'w') as fd:\n",
    "    # iterate through the Textract responses, looking for the LINE and WORD entries\n",
    "    for resp in pages:\n",
    "        for blk in resp['Blocks']:\n",
    "            if blk['BlockType'] in ['LINE', 'WORD']:\n",
    "                # if 'http' is found at the beginning of the line, we assume a new paragraph of text will be started\n",
    "                loc = blk['Text'].find('https')\n",
    "                if loc >= 0 and loc <= 2:\n",
    "                    fd.write('\\n')\n",
    "                else:\n",
    "                    fd.write('%s ' % blk['Text'])\n",
    "                    \n",
    "print('See results file: %s\\n' % textract_results_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the entire results set to a local file\n",
    "# this file isn't used in the remaining example, but you can open this JSON file in your Jupyter Notebook and view the elements returned by Textract\n",
    "with open(json_textract_results_filename, 'w') as fd:\n",
    "    json.dump(pages, fd)\n",
    "print(json_textract_results_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Amazon Comprehend\n",
    "Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to discover insights from text. The service provides APIs for Keyphrase Extraction, Sentiment Analysis, Entity Recognition, Topic Modeling, and Language Detection so you can easily integrate natural language processing into your applications. The following cells will walk through several examples of how to use the API.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the comprehend boto3 client (from the existing boto3 session)\n",
    "comp_client = session.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by loading the textract file that we will use for the next few examples\n",
    "lines = []\n",
    "with open(textract_results_filename) as fd:\n",
    "    lines = fd.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sentiment Analysis\n",
    "Use Amazon Comprehend to determine the Sentiment of each line of text from the Textract analysis.\n",
    "\n",
    "### Sync vs Async\n",
    "if you have text like may be small online review or one or two sentence text that doesn't exceed 5000bytes. you can run a lot of comprehend calls synchronously and by directly providing text string. However if your text exceeds that size you will have to make an async job, get its status and act on the output as you like. Below I show you both examples. Note the brevity of the sync calls compared to Async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example to demonstrate a synchoronous call to Comprehend to get the sentiment from a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_client.detect_sentiment(Text=\"I do not like grren eggs and ham\",\n",
    "                    LanguageCode='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take each line of text that we extracted from Textract and send it synchonously to Comprehend to get the sentiment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = {}\n",
    "batch_size = 25\n",
    "with open(comprehend_sentiments_results_filename, 'w') as fd:\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = [op[:4998] for op in lines[i:i+batch_size]]        \n",
    "        response = comp_client.batch_detect_sentiment(TextList=batch, LanguageCode='en')\n",
    "        for idx, line_result in enumerate(response['ResultList'], start=0):\n",
    "            sentiment = line_result['Sentiment']\n",
    "            if sentiment in sentiments:\n",
    "                sentiments[sentiment] += 1\n",
    "            else:\n",
    "                sentiments[sentiment] = 1\n",
    "            fd.write('%s|%s' % (sentiment, batch[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets calculate the sentiment frequencies \n",
    "\n",
    "freq = CalcFrequencies(sentiments)\n",
    "print('Frequencies:')\n",
    "for d in sentiments:\n",
    "    print('%s: %.2f' % (d, freq[d]))        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(trimmed_textract_results_filename, 'w') as fd:\n",
    "    for line in lines:\n",
    "        fd.write(line[:4998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_sentiment_input = \"s3://%s/%s\" % (bucket, os.path.basename(trimmed_textract_results_filename))\n",
    "s3_sentiment_output = \"s3://%s/%s\" % (bucket, \"sentiment_output\")\n",
    "! aws s3 cp {trimmed_textract_results_filename} {s3_sentiment_input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comp_client.start_sentiment_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': s3_sentiment_input,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': s3_sentiment_output\n",
    "    },\n",
    "    DataAccessRoleArn=comprehend_role,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "jobId = response['JobId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell takes 7 mins to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = comp_client.describe_sentiment_detection_job(JobId=jobId)\n",
    "while response['SentimentDetectionJobProperties']['JobStatus'] == 'IN_PROGRESS':\n",
    "    time.sleep(10)\n",
    "    response = comp_client.describe_sentiment_detection_job(JobId=jobId)\n",
    "response['SentimentDetectionJobProperties']['JobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3uri = response['SentimentDetectionJobProperties']['OutputDataConfig']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {s3uri} results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar zxvf results/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase Extraction\n",
    "Use Amazon Comprehend to extract Key Phrases in the text from the Textract analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep a running total of the various key phrases\n",
    "keyphrase_counts = {}\n",
    "\n",
    "for i in range(0, len(lines), batch_size):\n",
    "    batch = [op[:4998] for op in lines[i:i+batch_size]]        \n",
    "    response = comp_client.batch_detect_key_phrases(TextList=batch, LanguageCode='en')\n",
    "    for idx, line_result in enumerate(response['ResultList'], start=0):\n",
    "        for keyphrase in line_result['KeyPhrases']:\n",
    "            kp = keyphrase['Text']\n",
    "            if kp in keyphrase_counts:\n",
    "                keyphrase_counts[kp] += 1\n",
    "            else:\n",
    "                keyphrase_counts[kp] = 1\n",
    "\n",
    "sorted_keyphrase_counts = dict(sorted(keyphrase_counts.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the frequency of each key phrase\n",
    "freq = CalcFrequencies(sorted_keyphrase_counts)\n",
    "\n",
    "# the results file is in csv format and includes the raw counts and the frequency\n",
    "with open(comprehend_keyphrases_results_filename, 'w') as fd:\n",
    "    fd.write('key_phrase|count|frequency\\n')\n",
    "    for kp in sorted_keyphrase_counts:  \n",
    "        fd.write('%s|%d|%.4f\\n' % (kp, sorted_keyphrase_counts[kp], freq[kp]))\n",
    "\n",
    "print('See results file: %s' % (comprehend_keyphrases_results_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for kp in sorted_keyphrase_counts:  \n",
    "    i += 1\n",
    "    print('%s|%d|%.4f' % (kp, sorted_keyphrase_counts[kp], freq[kp]))\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Entity Recognition\n",
    "Use Amazon Comprehend to detect Entities in the text from the Textract analysis.  \n",
    "What are the type of Entities?\n",
    "* PERSON, ORGANIZATION, DATE, QUANTITY, LOCATION, TITLE, COMMERCIAL_ITEM, EVENT, OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "entities = {}\n",
    "\n",
    "with open(comprehend_entities_results_filename, 'w') as fd:\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = [op[:4998] for op in lines[i:i+batch_size]]        \n",
    "        response = comp_client.batch_detect_entities(TextList=batch, LanguageCode='en')\n",
    "        for idx, line_result in enumerate(response['ResultList'], start=0):\n",
    "            for entity in line_result['Entities']:\n",
    "                etype = entity['Type']\n",
    "                if etype in entities:\n",
    "                    entities[etype] += 1\n",
    "                else:\n",
    "                    entities[etype] = 1\n",
    "                fd.write('%s|%s\\n' % (etype, entity['Text']))\n",
    "                \n",
    "sorted_entities = dict(sorted(entities.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = CalcFrequencies(sorted_entities)\n",
    "print('Frequencies:')\n",
    "for d in sorted_entities:\n",
    "    print('%s: %.2f' % (d, freq[d]))        \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PII Entity Recognition\n",
    "Use Amazon Comprehend to detect PII Entities in the text from the Textract analysis.  \n",
    "What are the types of PII Entities?  \n",
    "* NAME, DATE-TIME, ADDRESS, USERNAME, URL, EMAIL, PHONE, CREDIT-DEBIT-EXPIRY, PASSWORD, AGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii_entities = {}\n",
    "\n",
    "with open(comprehend_pii_entities_results_filename, 'w') as fd:\n",
    "    for line in lines:\n",
    "        # maximum text length for Comprehend Entities is 5,000 characters\n",
    "        line = line[:4998]       \n",
    "        if len(line) > 1:\n",
    "            response = comp_client.detect_pii_entities(Text=line, LanguageCode='en')\n",
    "            for entity in response['Entities']:\n",
    "                etype = entity['Type']\n",
    "                if etype in pii_entities:\n",
    "                    pii_entities[etype] += 1\n",
    "                else:\n",
    "                    pii_entities[etype] = 1\n",
    "                fd.write('%s|%s\\n' % (etype, line[entity['BeginOffset']:entity['EndOffset']]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# sort the dictionary by values\n",
    "sorted_pii_entities = dict(sorted(pii_entities.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = CalcFrequencies(sorted_pii_entities)\n",
    "print('Frequencies:')\n",
    "for d in sorted_pii_entities:\n",
    "    print('%s: %.2f' % (d, freq[d]))        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
